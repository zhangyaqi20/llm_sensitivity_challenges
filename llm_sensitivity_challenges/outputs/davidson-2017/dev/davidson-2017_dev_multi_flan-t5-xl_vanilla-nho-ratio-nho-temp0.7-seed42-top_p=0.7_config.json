{"server": "epsilon", "data_name": "davidson-2017", "data_split": "dev", "data_size": 298, "label2index": {"hatespeech": 0, "offensive": 1, "normal": 2}, "label_col": "label_multi", "dataset_type": "multi", "label_counts": {"1": 231, "2": 50, "0": 17}, "llm_name": "flan-t5-xl", "task_name": "text2text-generation", "batch_size": 32, "top_p": 0.7, "seed_generation": 42, "top_k": 50, "typical_p": 1.0, "num_return_sequences": 1, "temperature": 0.7, "repetition_penalty": 1, "do_sample": true, "prompt_type": "vanilla-nho-ratio-nho-temp0.7-seed42-do_sample=True-top_p=0.7", "prompt_template": "Consider that the text originates from a dataset where 16.8% labels are normal, 5.8% labels are hatespeech, and 77.4% labels are offensive.\nBased on this classify the text delimited by three backticks as normal, hatespeech, or offensive. Provide the answer as either normal, hatespeech, or offensive only.\nExample output for normal : normal\nExample output for hatespeech : hatespeech\nExample output for offensive : offensive\n``` {text}```", "text_placeholder": "{text}", "predictions_filename": "outputs/davidson-2017/dev/davidson-2017_dev_multi_flan-t5-xl_vanilla-nho-ratio-nho-temp0.7-seed42-do_sample=True-top_p=0.7_predictions_raw.json", "success_rate": 1.0, "f1score": 0.7283634340142342, "f1score_label": {"hatespeech": 0.5116279069767442, "offensive": 0.9042316258351892, "normal": 0.7692307692307692}, "acc_score": 0.8523489932885906, "confusion_matrix_filename": "outputs/davidson-2017/dev/davidson-2017_dev_multi_flan-t5-xl_vanilla-nho-ratio-nho-temp0.7-seed42-do_sample=True-top_p=0.7_confusion_matrix.png", "TP": [11, 203, 40], "TN": [266, 52, 234], "FP": [15, 15, 14], "FN": [6, 28, 10], "FPR": {"hatespeech": 0.05338078291814947, "offensive": 0.22388059701492538, "normal": 0.056451612903225805}, "FPR_overall": 0.1112376642787669, "|FPR - FPR_i|_avg": 0.07509528849077234, "FNR": {"hatespeech": 0.35294117647058826, "offensive": 0.12121212121212122, "normal": 0.2}, "FNR_overall": 0.2247177658942365, "|FNR - FNR_i|_avg": 0.08548227371756785, "FPR - FNR": {"hatespeech": -0.2995603935524388, "offensive": 0.10266847580280417, "normal": -0.1435483870967742}, "|FPR - FNR|_avg": 0.1819257521506724, "pred_imb_rate": {"hatespeech": 1.5294117647058822, "offensive": 0.9437229437229437, "normal": 1.08}, "imb_rate_avg": 1.2230149307429394, "execution_time": 5223.787702322006}