{"server": "epsilon", "data_name": "hasoc-2019-multi", "data_split": "test", "data_size": 288, "label2index": {"hatespeech": 0, "profane": 1, "offensive": 2}, "label_col": "label_multi", "dataset_type": "multi", "label_counts": {"0": 124, "1": 93, "2": 71}, "llm_name": "flan-t5-xl", "task_name": "text2text-generation", "batch_size": 32, "top_p": 0.7, "seed_generation": 42, "top_k": 50, "typical_p": 1.0, "num_return_sequences": 1, "temperature": 0.7, "repetition_penalty": 1, "do_sample": true, "prompt_type": "vanilla-hpo-occurrence-123-temp0.7-top_p=0.7-seed42", "prompt_template": "Consider that the text originates from a dataset where hatespeech occurs more frequently than profane, profane occurs more frequently than offensive, and hatespeech occurs more frequently than offensive.\nBased on this classify the text delimited by three backticks as hatespeech, profane, or offensive. Provide the answer as either hatespeech, profane, or offensive only.\nExample output for hatespeech : hatespeech\nExample output for profane : profane\nExample output for offensive : offensive\n``` {text}```", "text_placeholder": "{text}", "predictions_filename": "outputs/hasoc-2019-multi/test/hasoc-2019-multi_test_multi_flan-t5-xl_vanilla-hpo-occurrence-123-temp0.7-top_p=0.7-seed42_predictions.json", "success_rate": 1.0, "recall": 0.5780200918774294, "recall_label": {"hatespeech": 0.5806451612903226, "profane": 0.9139784946236559, "offensive": 0.23943661971830985}, "|Rec - Rec_i|_avg": 0.2257223147727464, "f1score": 0.5540976064912235, "f1score_label": {"hatespeech": 0.6127659574468086, "profane": 0.7589285714285713, "offensive": 0.29059829059829057}, "acc_score": 0.6041666666666666, "confusion_matrix_filename": "outputs/hasoc-2019-multi/test/hasoc-2019-multi_test_multi_flan-t5-xl_vanilla-hpo-occurrence-123-temp0.7-top_p=0.7-seed42_confusionmatrix.png", "TP": [72, 85, 17], "TN": [125, 149, 188], "FP": [39, 46, 29], "FN": [52, 8, 54], "FPR": {"hatespeech": 0.23780487804878048, "profane": 0.2358974358974359, "offensive": 0.1336405529953917}, "FPR_overall": 0.20244762231386937, "|FPR - FPR_i|_avg": 0.045871379545651764, "FNR": {"hatespeech": 0.41935483870967744, "profane": 0.08602150537634409, "offensive": 0.7605633802816901}, "FNR_overall": 0.4219799081225705, "|FNR - FNR_i|_avg": 0.22572231477274637, "FPR - FNR": {"hatespeech": -0.18154996066089696, "profane": 0.1498759305210918, "offensive": -0.6269228272862984}, "|FPR - FNR|_avg": 0.3194495728227624, "pred_imb_rate": {"hatespeech": -0.10483870967741936, "profane": 0.40860215053763443, "offensive": -0.352112676056338}, "imb_rate_avg": 0.28851784542379727, "execution_time": 34.61414194107056}